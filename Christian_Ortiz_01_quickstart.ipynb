{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cortiz313/Machine-Learning-Class/blob/main/Christian_Ortiz_01_quickstart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnUQs11DflMR"
      },
      "source": [
        "# A quickstart introduction\n",
        "\n",
        "## An example ...\n",
        "As part of a [conservation effort](http://burrowingowlconservation.org/sightings/), Ann would like to report sightings of Burrowing Owls as she is hiking. Unfortunately, Ann doesn't know what a Burrowing Owl looks like so she goes to the web to look at pictures. What she has then, is a set of images that are labeled. By examining these labeled images she is **training** herself to recognize Burrowing Owls. \n",
        "\n",
        "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/owls2.png)\n",
        "\n",
        "\n",
        "More generally, we can call this set of labeled images used for training, the **labeled training dataset**. Let's dive into this idea of a labeled training dataset a bit more. Suppose Clara is given the task of distinguishing between pictures of telecaster style guitars and stratocaster. But not to worry, because her boss has given her thousands of pictures of guitars. When looking at a picture, the only thing Clara knows is that it is of either a stratocaster or a telecaster. For example, here are some pictures of stratocasters and telecasters. \n",
        "\n",
        "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/guitars2.png)\n",
        "\n",
        "\n",
        "Again, when looking at a picture all she knows is that it is a picture of a stratocaster or telecaster but she doesn't know which of the two it is. How long will it take for Clara to learn how to distinguish between these two guitar styles? Would the time be significantly shorter if her boss gave her 10,000 pictures or 100,000? If this is all the information she gets, she will never learn. What she needs is a **labeled** dataset. When presented with a picture she needs to know whether it is a picture of a stratocaster or a telecaster. \n",
        "\n",
        "\n",
        "**Now back to Ann learning to recognize Burrowing Owls**\n",
        "\n",
        "When Ann is learning to recognize Burrowing Owls from her labeled training set, she is developing a model of what features make it a Burrowing Owl. Once she is done learning she can be on a hike, see an animal, and classify it as a Burrowing Owl or something else. This is an **inference** process---based on the evidence of different features of the animal she can infer what it is. And to throw more jargon at you, this type of problem is called a **classification problem**. In classification problems the system is given the features of an object and it needs to classify that object. For example,\n",
        "\n",
        "* The features might be the words of a Twitter post (i.e., *Everything Everywhere All At Once was a f-ing masterpiece. I can't emphasize how great this movie is, it's just that great.*) and based on those features the system classifies the post as positve, negative, or neutral.\n",
        "* The features might be the pixels of an image and based on those pixels the system classifies the image into one of 1,000 categories (it's an image of an owl, a bicycle).\n",
        "\n",
        "\n",
        "\n",
        "## machine learning\n",
        "In machine learning, classification systems have a similar two step process. First is the training phase where the system uses a labeled training dataset to build a model. (We will learn about the architecture of these models and how they learn a bit later.) \n",
        "\n",
        "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/quickDiagram2.png)\n",
        "\n",
        "\n",
        "\n",
        "> **Supervised vs. Unsupervised learning**. When a machine learning system trains on labeled data this is called supervised learning. When a system learns with unlabeled data this is called unsupervised learning. A very common example of unsupervised learning is clustering where we might give our system a million unlabeled pictures and ask it to divide the images into 10 groups. \n",
        "\n",
        "Once we have a trained model, we can use that model for inference---we can give it pictures and the system can classify them as burrowing owl or something else. Again, the two phases are:\n",
        "\n",
        "1. training\n",
        "2. inference\n",
        "\n",
        "In this introduction we are going to ignore the training phase and learn a bit about inference. To do that we are going to use a pre-trained model. What *pre-trained model* simply means is that someone else designed the architecture of the model and trained it. \n",
        "\n",
        "### AlexNet\n",
        "\n",
        "The pretrained model we will use is AlexNet. AlexNet was designed by Alex Krishevsky, Ilya Sutskever, and Geoffrey Hinton. In 2012, AlexNet won a competition where the competing systems had to classify images into one of 1,000 categories. AlexNet had a 15% error rate and the error rate of the second place winner was over 25%. Since then there are dozens of systems that perform better, but we will use AlexNet because of its historic significance. The pretrained AlexNet was trained on a labeled training dataset of over one million images.\n",
        "\n",
        "## Let's get started\n",
        "\n",
        "#### First, a note ...\n",
        "The intent of this notebook is for you to learn a little bit about data mining and have a bit of fun. The idea is not for you to understand every line of code. That will come later.\n",
        "\n",
        "**Note:**\n",
        "\n",
        "**First let's set the runtime to GPU (Graphics Processing Unit) -- click on 'runtime' in the menu above, select 'Change runtime type' and pick 'GPU'.**\n",
        "\n",
        "Let's check to see if we set the runtime to GPU:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xh0M2g6Tl6C"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXl7t6HpT2jk"
      },
      "source": [
        "You should see something like:\n",
        "\n",
        "```\n",
        "+-----------------------------------------------------------------------------+\n",
        "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
        "|-------------------------------+----------------------+----------------------+\n",
        "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
        "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
        "|                               |                      |               MIG M. |\n",
        "|===============================+======================+======================|\n",
        "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
        "| N/A   64C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
        "|                               |                      |                  N/A |\n",
        "+-------------------------------+----------------------+----------------------+\n",
        "                                                                               \n",
        "+-----------------------------------------------------------------------------+\n",
        "| Processes:                                                                  |\n",
        "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
        "|        ID   ID                                                   Usage      |\n",
        "|=============================================================================|\n",
        "|  No running processes found                                                 |\n",
        "+-----------------------------------------------------------------------------+\n",
        "```\n",
        "\n",
        "Showing, for example, that we are using a Tesla T4 GPU\n",
        "\n",
        "Now we will set up a variable to allow us to use the GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "4NoR3PrrVdBD",
        "outputId": "c9fc221d-e51c-4aeb-9a65-3922bbed04ce"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():  \n",
        "  dev = \"cuda:0\" \n",
        "else:  \n",
        "  dev = \"cpu\"  \n",
        "device = torch.device(dev) \n",
        "dev"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbSyRnWYflMY"
      },
      "source": [
        "### 1. Install Pytorch Lightning.\n",
        "First, let's install the Pytorch Lightning library on our virtual machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pm_bhhMjflMY",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-lightning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7vaBQx2flMa"
      },
      "source": [
        "The exclamation point (aka *bang*) at the beginning of the line instructs the system to interpret the rest of the line as a Unix command (something you might type in a Unix terminal).\n",
        "\n",
        "For example\n",
        "\n",
        "```!ls```\n",
        "\n",
        "will list the contents of the current directory\n",
        "\n",
        "`pip` is the **p**ackage **i**nstaller for **P**ython. As the name suggests, it install Python libraries (packages) that are not already present in the system. In this case,\n",
        "\n",
        "In the case above, we are installing the `pytorch-lightning` library.\n",
        "\n",
        "\n",
        "### 2. Import the computer vision library\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jpMG96oTflMb"
      },
      "outputs": [],
      "source": [
        "from torchvision import models\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpQo-Pb6flMb"
      },
      "source": [
        "Without the bang Jupyter interprets code lines in this notebook as a Python commands which\n",
        "\n",
        "```from torchvision import models``` is.\n",
        "\n",
        "`torchvision` is a library containing models and other components for computer vision. `torch` is the basic PyTorch deep learning library.\n",
        "\n",
        "There are many pretrained models available to use. Let's take a look at the possibilities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNY3j0YqflMb"
      },
      "outputs": [],
      "source": [
        "dir(models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0IyX-BiflMc"
      },
      "source": [
        "That is a lot of pretrained models we can use!\n",
        "\n",
        "### 3. Load  the pretrained AlexNet model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxiDfvuKflMc"
      },
      "outputs": [],
      "source": [
        "alexnet = models.alexnet(weights='AlexNet_Weights.DEFAULT')\n",
        "# have model run on the GPU\n",
        "alexnet.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mu7BBAs0UoEd"
      },
      "source": [
        "And let's check to see if the model is using the GPU ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5WIqgEYUse9",
        "outputId": "82741d15-3dfb-4a14-ab07-e54d6af0851f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "next(alexnet.parameters()).is_cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yvDAoqNflMc"
      },
      "source": [
        "Excellent! CUDA is the API for NVIDIA GPUs that allow us to do parallel programming.\n",
        "\n",
        "Of course we could call this model anything we want:\n",
        "\n",
        "```\n",
        "alexnet_model = models.alexnet(pretrained=True)\n",
        "myModel = models.alexnet(pretrained=True)\n",
        "```\n",
        "\n",
        "Now we have a pretrained model loaded into our system. We would like to use the model to classify our own images. \n",
        "\n",
        "## Inference\n",
        "\n",
        "We would like to give AlexNet an image like:\n",
        "\n",
        "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/poodle.jpg)\n",
        "\n",
        "and have AlexNet classify it. First, let's download the image file from the web using the Unix command `curl`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXZzt6a6flMc"
      },
      "outputs": [],
      "source": [
        "!curl http://zacharski.org/files/courses/dmpics/poodle.jpg -o poodle.jpg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umQSlZbRflMd"
      },
      "source": [
        "Next, let's load that image into Python using PIL (Python Imaging Library):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2nI-yVPYflMd"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "img = Image.open(\"poodle.jpg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OTUlDW-flMd"
      },
      "source": [
        "Now let's display that image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3Mny1F_BflMe"
      },
      "outputs": [],
      "source": [
        "img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSdD1WZQflMe"
      },
      "source": [
        "(If that doesn't display an image change the code to `img.show`)\n",
        "\n",
        "\n",
        "The size of this particular image is 4032x3024 which is slightly over 12 million pixels. That is a lot of pixels! AlexNet was designed to work with an image size of 224x224. So we need to transform the original image to AlexNet specifications by using some methods from the `torchvision` library. \n",
        "\n",
        "First we will use `transforms.Resize(256)` which transforms the image so that the smaller dimension of the original image will be resized to 256.  ([PyTorch documentation](https://pytorch.org/vision/main/generated/torchvision.transforms.Resize.html)). The resultant image will be 341x256.\n",
        "\n",
        "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/poodleSmall.jpg)\n",
        "\n",
        "\n",
        "To get the image to the final 224x224 size we are going to use `transforms.CenterCrop(224)` which as the name suggests crops the image at the center to a 224x224 square. The result will look something like:\n",
        "\n",
        "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/poodleCropped.jpg)\n",
        "\n",
        "\n",
        "\n",
        "Then we will convert the image to an array using `ToTensor`. Since each pixel has values for red, green and blue (RGB), the resulting array will be 224x224x3. Finally we normalize the tensor using `transforms.Normalize` ([PyTorch Documentation](https://pytorch.org/vision/main/generated/torchvision.transforms.Normalize.html?highlight=normalize#torchvision.transforms.Normalize)). \n",
        "\n",
        "\n",
        "\n",
        "Here are those transformations put together:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6rmI2eXBflMe"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229,0.224,0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPmTNuADflMf"
      },
      "source": [
        "You may wonder where the numbers come from in \n",
        "\n",
        "```\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229,0.224,0.225])\n",
        "```\n",
        "These are the mean and standard deviation of the RGB values for all the pixels in all the images in the ImageNet dataset.\n",
        "\n",
        "Let's use this method we defined to transform the image:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Z3I_BAn9flMf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1773daa8-cdd7-4334-8817-49f1f23065db"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "img_t = transform(img)\n",
        "\n",
        "batch_t = torch.unsqueeze(img_t, 0)\n",
        "\n",
        "# put the tensor on the GPU\n",
        "batch_t = batch_t.to(device)\n",
        "batch_t.is_cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUHn9foEflMf"
      },
      "source": [
        "Image classification models typically classify an array of images at once rather than a single image. \n",
        "\n",
        "```\n",
        "    batch_t = torch.unsqueeze(img_t, 0)\n",
        "```\n",
        "creates a tensor with one element `img_t` which itself is a tensor. In a sense, an array of image arrays.\n",
        "\n",
        "Next,\n",
        "\n",
        "```\n",
        "batch_t = batch_t.to(device)\n",
        "```\n",
        "Puts that Tensor on the GPU and\n",
        "\n",
        "```\n",
        "batch_t.is_cuda\n",
        "```\n",
        "checks to make sure that that is the case.\n",
        "\n",
        "Now we have prepared the image and are prepared to pass it to alexnet for inference.\n",
        "\n",
        "### Model Inference\n",
        "\n",
        "In PyTorch, models can be in two modes and we can toggle between them.\n",
        "\n",
        "* `alexnet.eval()` puts the model in inference mode so it can make predictions.\n",
        "* `alexnet.train()` puts the model in training mode.\n",
        "\n",
        "\n",
        "Let's get the model ready for inference:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07sjKJ6CflMf"
      },
      "outputs": [],
      "source": [
        "alexnet.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H22BjQSlflMg"
      },
      "source": [
        "As you can see, `alexnet.eval()` displays a lot of information about the architecture of the model. We will learn about the architecture of deep learning models about midway through the course.\n",
        "\n",
        "Now let's pass the tensor of our image to alexnet and get the output. Plus, let's examine the shape of the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ksUu8iE1flMg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1887a3e-32a0-4f01-9eec-ff6ef551cdba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1000])\n"
          ]
        }
      ],
      "source": [
        "out = alexnet(batch_t)\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9WSfV73flMg"
      },
      "source": [
        "As we see `out` is a one dimensional tensor with 1,000 different values. We get 1,000 values because Image_net contained 1,000 labels for the images. The larger the number the more likely the image is of that class. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itIaAMZEflMh"
      },
      "outputs": [],
      "source": [
        "out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiH6cKiaflMh"
      },
      "source": [
        "Let's convert those values to probabilities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UORMvTUflMh"
      },
      "outputs": [],
      "source": [
        "percentage = torch.nn.functional.softmax(out, dim=1)[0] * 100\n",
        "percentage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP-kvbkmflMh"
      },
      "source": [
        "As you can see, the image is not very likely to be one of the first 5 labels. What are the actual names of these labels. First let's download the label name file:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmDUeY4kflMi"
      },
      "outputs": [],
      "source": [
        "!curl http://zacharski.org/files/courses/dmpics/imagenet_classes.txt -o imagenet_classes.txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjlBPVW9flMi"
      },
      "source": [
        "Let's load in those labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toeNd63OflMi"
      },
      "outputs": [],
      "source": [
        "with open('imagenet_classes.txt') as f:\n",
        "\n",
        "  labels = [line.strip() for line in f.readlines()]\n",
        "print(labels[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hee-TGdYflMi"
      },
      "source": [
        "The first five labels are of types of fish. It is good to know our model didn't think our image was of a fish. What does our model think?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLzeK4WIflMj"
      },
      "outputs": [],
      "source": [
        "z_, index = torch.max(out, 1)\n",
        "print(index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlPw0CvPflMj"
      },
      "source": [
        "Okay, the label at index 267 is the most likely. Let's print that out:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMDZWOmEflMj"
      },
      "outputs": [],
      "source": [
        "print(labels[index[0]], percentage[index[0]].item())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHH57CliflMj"
      },
      "source": [
        "Fortunately, alexnet correctly thinks the image is of a standard poodle with 38.65% likelihood. \n",
        "\n",
        "The ImageNet competition evaluated systems based on the top-5 error rate meaning the system was judged correct if the correct label was among the top 5 the system predicted.  Let's look at the top 5 our model predicted for this image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xL-_AsjXflMk"
      },
      "outputs": [],
      "source": [
        "_, indices = torch.sort(out, descending=True)\n",
        "[(labels[idx], percentage[idx].item()) for idx in indices[0][:5]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dEP9tLjflMk"
      },
      "source": [
        "Let's turn what we learned into a function and try predicting the class of other images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "O3BT8abIflMk"
      },
      "outputs": [],
      "source": [
        "import  requests\n",
        "\n",
        "def predict(url):\n",
        "    # first download the image from the web\n",
        "    r = requests.get(url)\n",
        "    with open('tmp.jpg', 'wb') as f:\n",
        "        f.write(r.content)\n",
        "    img = Image.open('tmp.jpg')\n",
        "    img.show()\n",
        "    img_t = transform(img)\n",
        "    batch_t = torch.unsqueeze(img_t, 0)\n",
        "    batch_t = batch_t.to(device)\n",
        "    out = alexnet(batch_t)\n",
        "    _, indices = torch.sort(out, descending=True)\n",
        "    percentage = torch.nn.functional.softmax(out, dim=1)[0] * 100\n",
        "    return([(labels[idx], percentage[idx].item()) for idx in indices[0][:5]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbXBwIfxflMk"
      },
      "source": [
        "Let's find out what the 1,000 labels are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCsSr4vjflMl"
      },
      "outputs": [],
      "source": [
        "\n",
        "line = ''\n",
        "for i in range(len(labels)):\n",
        "  line += '%-25s' %labels[i]\n",
        "  if (i + 1) % 4 == 0:\n",
        "    print(line)\n",
        "    line = ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYkTXbujflMl"
      },
      "source": [
        "Electric Guitar is one of the labels. Let's see if our model correctly identifies a picture of one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUjOor_oflMl",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "predict('https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/Fender_Stratocaster.jpeg')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8msErztflMl"
      },
      "source": [
        "Our system is 99% sure that this image is an electric guitar.\n",
        "\n",
        "While AlexNet doesn't know about burrowing owls it does know about great horned owls. Let's give it a picture of a burrowing owl and see what it does:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUMAPhl-flMm"
      },
      "outputs": [],
      "source": [
        "predict('https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/greyOwl.jpeg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vkazE55flMm"
      },
      "source": [
        "That is a reasonable response!\n",
        "\n",
        "Let's try a cello."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bKIiLdNflMm"
      },
      "outputs": [],
      "source": [
        "predict('https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/cello12.jpeg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCLaql64flMm"
      },
      "source": [
        "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/torchdivide.png)\n",
        "\n",
        "\n",
        "# <font color='#EE4C2C'>You Try ...</font> \n",
        "Ok, it is time for you to try out what you just learned.\n",
        "\n",
        "## <font color='#EE4C2C'>1. Your own images</font> \n",
        "Try this out on three images of your own."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "GanSdKhSflMn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39141c58-e961-4c8f-8974-7b8efb00b3d1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Siberian husky', 13.124492645263672),\n",
              " ('dingo', 12.321903228759766),\n",
              " ('wire-haired fox terrier', 11.739368438720703),\n",
              " ('Eskimo dog', 11.098215103149414),\n",
              " ('borzoi', 8.581761360168457)]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# Small Dog\n",
        "predict('https://static.wixstatic.com/media/1556ee_d0d16fca7f7647d98b13925809ae4500~mv2.jpg/v1/crop/x_579,y_0,w_3449,h_3442/fill/w_256,h_256,al_c,q_80,usm_0.66_1.00_0.01,enc_auto/P6250744_JPG.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "PoHaOAiZflMn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "095e7003-6bcd-4036-b178-5225d221569d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('starfish', 43.36981201171875),\n",
              " ('pretzel', 32.398773193359375),\n",
              " ('hook', 11.302626609802246),\n",
              " ('knot', 4.323400497436523),\n",
              " ('chain', 3.29124116897583)]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# Octopus\n",
        "predict('https://m.media-amazon.com/images/I/81Jn3ynx-ZL._CR0,204,1224,1224_UX256.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "xhck8yneflMn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce51816f-11a9-4cd3-a16a-311926994456"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('lion', 99.8720703125),\n",
              " ('dhole', 0.045167431235313416),\n",
              " ('chow', 0.033557455986738205),\n",
              " ('coyote', 0.023083439096808434),\n",
              " ('timber wolf', 0.014799346216022968)]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "# Lion\n",
        "predict('https://d29rinwu2hi5i3.cloudfront.net/article_media/be642a8c-a3f7-4389-905b-b7dc18db52fd/headline_lion_2.jpg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbFQ20DUflMn"
      },
      "source": [
        "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/PyDivideTwo.png)\n",
        "## <font color='#EE4C2C'>2. Squeezenet</font> \n",
        "\n",
        "Let's try a different pretrained model, `squeezenet1_1`. Load the model, construct a function that will make predictions based on the model and try it out on the images above that we provided plus your three.</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57or8T7-flMn"
      },
      "outputs": [],
      "source": [
        "# Setting up the model\n",
        "squeezeModel = models.squeezenet1_1(weights='SqueezeNet1_1_Weights.DEFAULT')\n",
        "\n",
        "squeezeModel.cuda()\n",
        "\n",
        "next(squeezeModel.parameters()).is_cuda\n",
        "\n",
        "squeezeModel.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The function to predict with the SqueezeNet model (predict2)\n",
        "import  requests\n",
        "\n",
        "def predict2(url):\n",
        "    \n",
        "    r = requests.get(url)\n",
        "    with open('tmp.jpg', 'wb') as f:\n",
        "        f.write(r.content)\n",
        "    img = Image.open('tmp.jpg')\n",
        "    img.show()\n",
        "    img_t = transform(img)\n",
        "    batch_t = torch.unsqueeze(img_t, 0)\n",
        "    batch_t = batch_t.to(device)\n",
        "    out = squeezeModel(batch_t)\n",
        "    _, indices = torch.sort(out, descending=True)\n",
        "    percentage = torch.nn.functional.softmax(out, dim=1)[0] * 100\n",
        "    return([(labels[idx], percentage[idx].item()) for idx in indices[0][:5]])"
      ],
      "metadata": {
        "id": "oO6-HlQsqw8f"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Guitar (given) SqueezeNet\n",
        "predict2('https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/Fender_Stratocaster.jpeg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAyHSKUupa_j",
        "outputId": "5dc7e97c-c446-4094-d447-b87e197a7c1e"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('electric guitar', 98.95701599121094),\n",
              " ('acoustic guitar', 1.034130334854126),\n",
              " ('banjo', 0.0031488786917179823),\n",
              " ('violin', 0.001232030219398439),\n",
              " ('hook', 0.0010491503635421395)]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Guitar (given) AlexNet\n",
        "predict('https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/Fender_Stratocaster.jpeg')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APOJIHSTHmIG",
        "outputId": "94b53a6f-15ee-47bf-eb30-597f0a9c126c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('electric guitar', 99.0139389038086),\n",
              " ('acoustic guitar', 0.9524719715118408),\n",
              " ('rule', 0.019165046513080597),\n",
              " ('banjo', 0.0072016301564872265),\n",
              " ('hatchet', 0.0007767516653984785)]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "7eM_-FIdflMn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd27e954-963a-44b6-97d0-055e60b0e280"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('great grey owl', 82.93092346191406),\n",
              " ('ringlet', 10.850360870361328),\n",
              " ('puffer', 2.5810976028442383),\n",
              " ('African chameleon', 1.121286153793335),\n",
              " ('indri', 0.5488015413284302)]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "# Owl (given) SqueezeNet\n",
        "predict2('https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/greyOwl.jpeg')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Owl (given) AlexNet\n",
        "predict('https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/greyOwl.jpeg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCg8oBdWH28F",
        "outputId": "861c1941-4472-4895-cdd1-7e3b1e0afb92"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('great grey owl', 72.3724136352539),\n",
              " ('hare', 20.254365921020508),\n",
              " ('kite', 4.006711959838867),\n",
              " ('bittern', 1.0817829370498657),\n",
              " ('macaw', 0.6460889577865601)]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "5wxkMyuAflMo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7088058c-886e-4a2f-9f66-6efaa5c1ee26"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('cello', 65.8322982788086),\n",
              " ('violin', 34.16702651977539),\n",
              " ('acoustic guitar', 0.000545207760296762),\n",
              " ('hook', 6.222772935871035e-05),\n",
              " ('electric guitar', 3.828113040071912e-05)]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "# Cello (given) SqueezeNet\n",
        "predict2('https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/cello12.jpeg')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cello (given) AlexNet\n",
        "predict('https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/cello12.jpeg')"
      ],
      "metadata": {
        "id": "JlfUoJhtIIP2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e49c620-fca8-4d60-da02-2072a068f8bb"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('cello', 75.61215209960938),\n",
              " ('violin', 24.38234519958496),\n",
              " ('acoustic guitar', 0.0035001984797418118),\n",
              " ('banjo', 0.0019424731144681573),\n",
              " ('punching bag', 1.923392483149655e-05)]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "7oglJGX8flMo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a30f749-3f69-449b-9f86-c72e886e7b15"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('wire-haired fox terrier', 56.12382125854492),\n",
              " ('Lakeland terrier', 7.058378219604492),\n",
              " ('tennis ball', 5.255655765533447),\n",
              " ('Irish terrier', 5.170267581939697),\n",
              " ('borzoi', 3.208376407623291)]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "# Small Dog SqueezeNet\n",
        "predict2('https://static.wixstatic.com/media/1556ee_d0d16fca7f7647d98b13925809ae4500~mv2.jpg/v1/crop/x_579,y_0,w_3449,h_3442/fill/w_256,h_256,al_c,q_80,usm_0.66_1.00_0.01,enc_auto/P6250744_JPG.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Small Dog AlexNet\n",
        "predict('https://static.wixstatic.com/media/1556ee_d0d16fca7f7647d98b13925809ae4500~mv2.jpg/v1/crop/x_579,y_0,w_3449,h_3442/fill/w_256,h_256,al_c,q_80,usm_0.66_1.00_0.01,enc_auto/P6250744_JPG.jpg')"
      ],
      "metadata": {
        "id": "kBtQll-xITZT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22782e10-a04b-45bb-adae-42f98d8075ed"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Siberian husky', 13.124492645263672),\n",
              " ('dingo', 12.321903228759766),\n",
              " ('wire-haired fox terrier', 11.739368438720703),\n",
              " ('Eskimo dog', 11.098215103149414),\n",
              " ('borzoi', 8.581761360168457)]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "05iaa3rhflMo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93abf265-7389-479f-cbfe-a6d7096b987e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('starfish', 89.46566009521484),\n",
              " ('knot', 1.8542699813842773),\n",
              " ('chain', 1.3044801950454712),\n",
              " ('pretzel', 1.156561017036438),\n",
              " ('night snake', 1.1037392616271973)]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "# Octopus SqueezeNet\n",
        "predict2('https://m.media-amazon.com/images/I/81Jn3ynx-ZL._CR0,204,1224,1224_UX256.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Octopus AlexNet\n",
        "predict('https://m.media-amazon.com/images/I/81Jn3ynx-ZL._CR0,204,1224,1224_UX256.jpg')"
      ],
      "metadata": {
        "id": "F8QjsBATIY1t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c064817f-6d20-4415-f6df-08c2768605dd"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('starfish', 43.36981201171875),\n",
              " ('pretzel', 32.398773193359375),\n",
              " ('hook', 11.302626609802246),\n",
              " ('knot', 4.323400497436523),\n",
              " ('chain', 3.29124116897583)]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "yAgCjkvwflMo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5584ef4f-b993-46b5-944f-10bd1d21527c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('lion', 99.99097442626953),\n",
              " ('chow', 0.005395764485001564),\n",
              " ('red wolf', 0.002386545529589057),\n",
              " ('dhole', 0.0008315872983075678),\n",
              " ('coyote', 0.00019514700397849083)]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "# Lion SqueezeNet\n",
        "predict2('https://d29rinwu2hi5i3.cloudfront.net/article_media/be642a8c-a3f7-4389-905b-b7dc18db52fd/headline_lion_2.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lion AlexNet\n",
        "predict('https://d29rinwu2hi5i3.cloudfront.net/article_media/be642a8c-a3f7-4389-905b-b7dc18db52fd/headline_lion_2.jpg')"
      ],
      "metadata": {
        "id": "zbkdqHa7Ic2h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0af4527-2c36-4e0e-baa6-d9222aec5509"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('lion', 99.8720703125),\n",
              " ('dhole', 0.045167431235313416),\n",
              " ('chow', 0.033557455986738205),\n",
              " ('coyote', 0.023083439096808434),\n",
              " ('timber wolf', 0.014799346216022968)]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqdx9MG8flMo"
      },
      "source": [
        "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/PyDivideTwo.png)\n",
        "## <font color='#EE4C2C'>2. Summary</font> \n",
        "\n",
        "Please answer the following questions by editing this markdown cell. </span>\n",
        "\n",
        "1. Classification machine learning models have two modes. What are they?  \n",
        "<font color='#EE4C2C'> Models can be in eval() mode, which prepares it to evaluate and predict what an image is, and train() mode, which is used to train the model. </font> \n",
        "2. What is a pretrained model? <font color='#EE4C2C'>A pretrained model is a model that has already been fed data and has been through the training process. This model is prepared to make an inference and a prediction on a new set of data.</font> \n",
        "2. What is supervised learning? What is unsupervised learning? \n",
        "<font color='#EE4C2C'> Supervised learning is when a machine learning system trains using labeled data. Unsupervised learning is when a system trains using unlabeled data. </font> \n",
        "3. Describe in a few sentences what Squeezenet is. (requires a bit of googling)\n",
        "<font color='#EE4C2C'> Squeezenet is a deep neural network for computer vision that was released in 2016 from researchers at DeepScale, University of California Berkeley, and Stanford. The goal of the creators of SqueezeNet was to create a smaller neural network with equivalent accuracy to the larger ones with fewer parameters so that it can more appropriately fit into computer memory and can more easily be transmitted over a computer network. Upon release, SqueezeNet was described as having \"AlexNet-level accuracy with 50x fewer parameters and < 0.5MB model size.\" Where AlexNet has 240MB of parameters, SqueezeNet has only 5MB. However, it is important to note that SqueezeNet is not just a 'squeezed' version of AlexNet, as they have entirely different architectures from one another.   </font> \n",
        "4. Compare the performance of AlexNet over Squeezenet. Was one more accurate than the other? Did you notice any other differences? \n",
        "<font color='#EE4C2C'> Overall, in my tests I would have to say that I found SqueezeNet to be more accurate for the data it was given. To be fair, in a few cases AlexNet was better. I found AlexNet to be about 1% more accurate at predicting the electric guitar, and 10% more accurate at predicting the cello. SqueezeNet was 10% more confident in its owl prediction, although as you mentioned in the lecture it is not exactly the same type of owl from their labels. However, the real differences came when I performed the test on the pictures I found myself. For my small dog picture, SqueezeNet predicted that it was a Wire-Haired Fox Terrier with 56% confidence, whereas AlexNet predicted that it was a Siberian Husky with 13% confidence. If you look at the picture, you will see that it is very likely to be a terrier of some kind, and looks nothing like a Husky. AlexNet did give it an 11% chance of being a terrier too, but again the SqueezeNet was much more accurate. For my Octopus picture, SqueezeNet was also more accurate, although neither had the appropriate label. SqueezeNet was 89% confident it was a Starfish, and AlexNet was 43% confident of the same thing, however AlexNet also gave it a 32% chance of being a pretzel, which made me laugh. For my final picture, both models were extremely accurate and had a nearly identical 99.9% confidence that it was a lion, which it was. If we take into account the reduced model size and how much fewer parameters SqueezeNet requires, it would seem like SqueezeNet is a much better choice for predicting the type of data that we did in these experiments. </font> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0aK-Aw_flMo"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}